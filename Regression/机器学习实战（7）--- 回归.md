# 机器学习实战（7）--- 回归

## 线性回归

### 原理

- 对连续型数据做出处理，回归的目的是预测数值型数据的目标值
- 假定输入数据存放在矩阵X中，回归系数存放在向量w中，对于给定的数据X1，预测结果将会通过Y = (X1^T)w，我们已知X和y，目标是通过已知的X与y求出w
- 常用的方法是找出使误差最小的w，此处的误差指的是预测的y值与真实的y值之间的差值，采用平方误差进行计算即最小二乘法
  1. 平方误差可以写作：$$ \sum ^{m}_{i=1}\left( y_{i}-x^{T}_{i}w\right) ^{2} $$ 
  2. 矩阵形式可以写作$$ \left( y-Xw\right) ^{T}\left( y-Xw\right) $$ 
  3. 对w进行求导，得到$$ X^{T}\left( Y-Xw\right) $$ 令其等于0，解出w
  4. $$ \widehat {w}(估计的w的最优解)=\left( X^{T}X\right) ^{-1}X^{T}y$$
  5. 此公式需要对矩阵求逆，因此旨在逆矩阵存在的时候适用，所以需要对矩阵是否可逆做出判断，判断的方法是：矩阵的行列式是否为0，不为0才有逆矩阵

- 线性回归方案：
  1. 读入数据，将数据特征值x，标签值y储存在矩阵x,y中
  2. 验证X^TX矩阵是否可逆
  3. 使用最小二乘求得回归系数w的最佳估计

### 代码

- 数据导入函数和标准回归函数

```python
from numpy import *

def load_data_set(file_name):
    '''
    加载数据
    解析以tab键分隔的文件中的浮点数
    :param file_name:输入数据
    :return:
        dataMat ：  feature 对应的数据集
        labelMat ： feature 对应的类别标签
    '''
    # 获取样本特征的总数
    num_feat = len(open(file_name).readline().split('\t')) -1
    data_mat = []
    label_mat = []
    fr = open(file_name)
    for line in fr.readlines():
        # 读取每一行
        line_arr = []
        cur_line = line.strip().split('\t')
        for i in range(num_feat):
            line_arr.append(float(cur_line[i]))
        data_mat.append(line_arr)
        label_mat.append(float(cur_line[-1]))
    return data_mat, label_mat

def stand_regres(x_arr, y_arr):
    '''
    线性回归
    :param x_arr:输入的样本数据，包含每个样本数据的 feature
    :param y_arr:对应于输入数据的类别标签，也就是每个样本对应的目标变量
    :return:
        ws：回归系数
    '''
    x_mat = mat(x_arr)
    y_mat = mat(y_arr).T
    xTx = x_mat.T * x_mat
    # 因为要用到xTx的逆矩阵，所以事先需要确定计算得到的xTx是否可逆，条件是矩阵的行列式不为0
    # linalg.det() 函数是用来求得矩阵的行列式的，如果矩阵的行列式为0，则这个矩阵是不可逆的，就无法进行接下来的运算
    if linalg.det(xTx) == 0.0:
        print("This matrix is singular, cannot do inverse")
        return
    ws = xTx.I * (x_mat.T * y_mat)
    return ws

def stand_regres_plot():
    '''
    画图
    :return: 拟合线性图
    '''
    # x_arr: 从load_data_set得到的特征数组
    # y_arr:从load_data_set得到的特征数组
    x_arr, y_arr = load_data_set('ex0.txt')
    x_mat = mat(x_arr)
    y_mat = mat(y_arr)
    # ws: 用stand_regres计算得到的回归系数
    ws = stand_regres(x_arr, y_arr)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    # 绘制原始数据点
    ax.scatter(x_mat[:, 1].flatten().A[0], y_mat.T[:, 0].flatten().A[0])
    # 如果直线上的点次序混乱，绘图将会出现问题，所以对直线上的点按照升序进行排序
    x_copy = x_mat.copy()
    x_copy.sort(0)
    y_hat = x_copy * ws
    ax.plot(x_copy[:, 1], y_hat)
    plt.show()
```

- 结果：![](https://www.z4a.net/images/2018/07/11/1.png)

## 局部加权线性回归

### 原理

- 线性回归可能出现欠拟合现象，局部加权是为了解决这个问题

- 主要思想是给待遇测点附近的每一个点赋予一定的权重，然后在这个子集上基于最小均方差进行普通的回归

- 这个算法每次预测仅需要事先选取出对应的数据子集，回归系数为$$ \widehat {w}=\left( X^{T}WX\right) ^{-1}X^{T}Wy$$ 其中W是一个矩阵用来给每个点赋予权重

- 局部加权线性回归使用“核”来对附近的点赋予更高的权重；核的类型可以自由选择，最常用的是高斯核，高斯核对应的权重如下：

  $$ w\left( i,i\right) =\exp \left( \dfrac {\left| x^{\left( i\right) }-x\right| }{-2k^{2}}\right) $$

- 这样就构建了一个只包含对角元素的权重矩阵w，并且点x与x(i)越近，w(i,i)的值将会越大；参数k决定了对附近点赋予多大的权重

### 代码

- 局部加权线性回归函数

```python
def lwlr(test_point, x_arr, y_arr, k =0.1):
    '''
    局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归
    :param test_point:测试点
    :param x_arr:样本的特征数据，即 feature
    :param y_arr:每个样本对应的类别标签，即目标变量
    :param k:关于赋予权重矩阵的核的一个参数，与权重的衰减速率有关；其中k是带宽参数，控制w（钟形函数）的宽窄程度，类似于高斯函数的标准差
    :return:
        testPoint * ws：数据点与具有权重的系数相乘得到的预测点

    note:
    高斯权重的公式，w = e^((x^((i))-x) / -2k^2) x为某个预测点，x^((i))为样本点
    样本点距离预测点越近，w越大，贡献的误差越大（权值越大），越远则贡献的误差越小（权值越小）
    '''
    x_mat = mat(x_arr)
    y_mat = mat(y_arr).T
    # 获得xMat矩阵的行数
    m = shape(x_mat)[0]
    # eye()返回一个对角线元素为1，其他元素为0的二维数组，该矩阵为每个样本点初始化了一个权重
    weights = mat(eye((m)))
    for j in range(m):
        diff_mat = test_point - x_mat[j, :]
        weights[j, j] = exp(diff_mat * diff_mat.T / (-2.0*k**2))
    xTx = x_mat.T * (weights * x_mat)
    if linalg.det(xTx) == 0.0:
        print("This matrix is singular, cannot do inverse")
        return
    ws = xTx.I * (x_mat.T * (weights * y_mat))
    return test_point * ws

def lwlr_test(test_arr, x_arr, y_arr, k = 0.1):
    '''
    对数据集中每个点调用 lwlr() 函数
    :param test_arr:测试样本点
    :param x_arr:样本的特征数据，即 feature
    :param y_arr:每个样本对应的类别标签
    :param k:控制核函数的衰减速率
    :return:预测点的估计值
    '''
    # 得到样本点的总数
    m = shape(test_arr)[0]
    y_hat = zeros(m)
    # 循环所有的数据点，并将lwlr运用于所有的数据点
    for i in range(m):
        y_hat[i] = lwlr(test_arr[i], x_arr, y_arr, k)
    # 返回估计值
    return y_hat

def lwlr_plot():
    x_arr, y_arr = load_data_set('ex0.txt')
    y_hat = lwlr_test(x_arr, x_arr, y_arr, 0.003)
    x_mat = mat(x_arr)
    # argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)
    srt_ind = x_mat[:, 1].argsort(0)
    x_sort = x_mat[srt_ind][:, 0, :]
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(x_sort[:, 1], y_hat[srt_ind])
    ax.scatter(x_mat[:, 1].flatten().A[0], mat(y_arr).T.flatten().A[0], s=2, c='red')
    plt.show()
```

- 结果![](https://www.z4a.net/images/2018/07/11/2.png)

## 岭回归

### 原理

- 数据的特征比样本点还多，就不能用前面的线性回归来做预测了，因为在计算(XTX)^-1会出错；因此引入了岭回归这种缩减方法
- 简单来说岭回归就是在矩阵XTX上加入一个λI 从而使得矩阵非奇异，然后就可以对XTX+λI求逆；其中矩阵I是一个 n * n （等于列数） 的单位矩阵， 对角线上元素全为1，其他元素全为0；而λ是一个用户定义的数值
- 回归系数的公式就变为：$$  \widehat {w}=\left( X^{T}X+λI\right) ^{-1}X^{T}Wy $$
- 岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计；这里通过引入 λ 来限制了所有 w 之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫作 缩减(shrinkage)

### 代码

- 岭回归

```python
def ridge_regres(x_mat, y_mat, lam = 0.2):
    '''
    这个函数实现了给定 lambda 下的岭回归求解
    如果数据的特征比样本点还多，就不能再使用上面介绍的的线性回归和局部现行回归了，因为计算 (xTx)^(-1)会出现错误
    如果特征比样本点还多（n > m），也就是说，输入数据的矩阵x不是满秩矩阵,非满秩矩阵在求逆时会出现问题
    :param x_mat:特征数据
    :param y_mat:类别标签
    :param lam:引入的一个λ值，使得矩阵非奇异；xTx + λI(I是对角线为1的单位矩阵)
    :return:经过岭回归计算得到的回归系数
    '''
    xTx = x_mat.T * x_mat
    # 岭回归就是在矩阵 xTx 上加一个 λI 从而使得矩阵非奇异，进而能对 xTx + λI 求逆
    denom = xTx + eye(shape(x_mat)[1]) * lam
    # 检查行列式linalg.det()是否为零，即矩阵是否可逆，行列式为0的话就不可逆，不为0的话就是可逆
    if linalg.det(denom) == 0.0:
        print("This matrix is singular, cannot do inverse")
        return
    ws = denom.I * (x_mat.T * y_mat)
    return ws

def ridge_test(x_arr, y_arr):
    '''
    函数 ridgeTest() 用于在一组 λ 上测试结果
    :param x_arr:
    :param y_arr:
    :return:
    '''
    x_mat = mat(x_arr)
    y_mat = mat(y_arr).T
    # 对各列求均值
    y_mean = mean(y_mat, 0)
    # Y的所有的特征减去均值
    y_mat = y_mat - y_mean
    # 标准化 x，计算 xMat 平均值
    x_means = mean(x_mat, 0)
    # 然后计算 X的方差
    x_var = var(x_mat, 0)
    # 数据标准化，所有特征都减去各自的均值并除以方差
    x_mat = (x_mat - x_means) / x_var
    # 可以在 30 个不同的 lambda 下调用 ridgeRegres() 函数
    num_test_pts = 30
    # 创建30 * m 的全部数据为0 的矩阵
    w_mat = zeros((num_test_pts, shape(x_mat)[1]))
    for i in range(num_test_pts):
        ws = ridge_regres(x_mat, y_mat, exp(i - 10))
        w_mat[i, :] = ws.T
    return w_mat

def ridge_plot():
    abX, abY = load_data_set('abalone.txt')
    ridge_weights = ridge_test(abX, abY)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(ridge_weights)
    plt.show()
```

- 结果![](https://www.z4a.net/images/2018/07/11/3.png)
- 这是回归系数(y轴)与log(λ)(x轴)的关系；λ非常小的时候系数与普通回归一样；λ非常大时，所有系数缩减为0；需要在中间找到使得预测结果最好的λ值
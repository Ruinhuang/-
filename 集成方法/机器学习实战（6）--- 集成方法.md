# 机器学习实战（6）--- 集成方法

## 概念

- 通俗来说： 类似于：当做重要决定时，大家可能都会考虑吸取多个专家而不只是一个人的意见。
- 分类
  1. 投票选举（bagging): 基于数据随机重抽样分类器构造方法（随机森林）
  2. 在学习(boosting):基于错误经验（所有分类器加权求和方法）（AdaBoost)

## 随机森林

- 随机森林指的是利用多棵树对样本进行训练并预测的一种分类器

### 原理

- 数据的随机化
  1. 采取有放回的抽样方式 构造子数据集，保证不同子集之间的数量级一样（不同子集／同一子集 之间的元素可以重复）
  2. 利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果
  3. 然后统计子决策树的投票结果，得到最终的分类 就是 随机森林的输出结果
- 待选特征随机化
  1. 子树从所有的待选特征中随机选取一定的特征
  2. 在选取的特征中选取最优的特征

### 算法特点

- 优点：几乎不需要输入准备、可实现隐式特征选择、训练速度非常快、其他模型很难超越、很难建立一个糟糕的随机森林模型、大量优秀、免费以及开源的实现
- 缺点：劣势在于模型大小、是个很难去解释的黑盒子
- 适用数据范围：数值型和标称型 

## AdaBoost

### 原理

- 使用弱分类器和多个实例构建一个强分类器

- 过程

  1. 数据中每个样本初始化相同的权重

  2. 训练一个弱分类器，计算该分类器的错误率，然后在同一数据集上再次训练弱分类器

  3. 在第二次训练的过程中，调整每个样本的权重

  4. 第一次分类正确的样本权重降低，分类错误的样本权重提高

  5. 为了从所有的弱分类器上得到最终的结果，AdaBoost为每个分类器分配了一个权重alpha

     - $$ \varepsilon (错误率)=\dfrac {未正确分类的样本数目}{所有样本数目} $$

     - $$ alpha=\dfrac {1}{2}\ln \left( \dfrac {1- \varepsilon}{\varepsilon}\right) $$

![](https://www.z4a.net/images/2018/07/11/15312811531.png)

### 算法特点

- 优点：泛化（由具体的、个别的扩大为一般的）错误率低，易编码，可以应用在大部分分类器上，无参数调节
-  缺点：对离群点敏感